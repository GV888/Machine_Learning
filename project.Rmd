---
title: "Practical Machine Learning project"
output: html_document

---

This is Practical Machine Learning project.

At first we load the necessary libraries:

```{r warning=FALSE, message=FALSE}

library(caret); library(kernlab);library(doParallel);
```

Then we load the trainings data:
```{r}
  training <- read.csv("c:/Users/Viktor/workspace/coursera/Machine_Learning/pml-training.csv", na.strings=c("#DIV/0!","NA",""))
  evaluation <- read.csv("c:/Users/Viktor/workspace/coursera/Machine_Learning/pml-testing.csv", na.strings=c("#DIV/0!","NA",""))
```

Subset the data for trainig and skip username and date rows
```{r}
feature_set <- training[,6:160]
evaluation_set <- evaluation[,6:160]
```


<h3>Preprocess the data</h3>
Convert all features to numeric and remove all rows with missing data
```{r}
feature_set<-feature_set[,colSums(is.na(feature_set)) == 0]
evaluation_set<-evaluation_set[,colSums(is.na(evaluation_set)) == 0]

for(i in c(1:ncol(feature_set))) {feature_set[,i] = as.numeric((feature_set[,i]))}
for(i in c(1:ncol(evaluation_set))) {evaluation_set[,i] = as.numeric((evaluation_set[,i]))}


```


Split the data to training and testing sets
```{r}
set.seed(96)
idx <- createDataPartition(y=feature_set$classe, p=0.75, list=FALSE )
training_set <- feature_set[idx,]
testing_set <- feature_set[-idx,]
```


Use Random Forests
```{r warning=FALSE, message=FALSE}
registerDoParallel()
modelFit <- train(y=as.factor(training_set$classe), x=training_set[-ncol(training_set)], tuneGrid=data.frame(mtry=3), trControl=trainControl(method="none"), method="parRF")
```

<h3>Cross-validation</h3>
The result will be placed in a confusion matrix to determinant the classifications accuracy.
```{r}
prediction <- predict(modelFit, newdata=testing_set)
confusionMatrix(prediction, testing_set$classe)
```
<h3>Conclusions</h3>
This model yielded a 99.86% prediction accuracy. Again, this model proved very robust and adequete to predict new data.


